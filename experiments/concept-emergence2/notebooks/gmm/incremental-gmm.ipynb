{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "import scipy.integrate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(k, dim, points_per_cluster, lim, spread):\n",
    "    # select n_clusters by sampling from a uniform distribution\n",
    "    centers = np.random.uniform(lim[0], lim[1], (k, dim))\n",
    "    # select cluster_std by sampling from a uniform distribution\n",
    "    cluster_std = np.random.uniform(spread[0], spread[1], k)\n",
    "    # make blobs with sklearn\n",
    "    X, y = make_blobs(n_samples=points_per_cluster, cluster_std=cluster_std, centers=centers, n_features=dim, random_state=1)\n",
    "    return X, y\n",
    "\n",
    "def plot_data(x, y=None):\n",
    "    dim = x.shape[1]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "\n",
    "    if y is None:\n",
    "\n",
    "        if (dim == 1):\n",
    "            ax.scatter(x, np.zeros_like(x), s=10, alpha=0.4)\n",
    "            #ax.set_xlim(-0.1, 1.1)\n",
    "            ax.set_ylim(-0.05, 1)\n",
    "        if(dim == 2):\n",
    "            ax.scatter(x[:,0], x[:,1], s=3, alpha=0.4)\n",
    "            ax.autoscale(enable=True)\n",
    "    else:\n",
    "        k_clusters = len(np.unique(y))\n",
    "        if dim == 1:\n",
    "            for i in range(k_clusters):\n",
    "                ax.scatter(x[y == i], np.zeros_like(x[y == i]), s=10, label=f\"Cluster{i}\", alpha=0.4)\n",
    "            # ax.set_xlim(-0.1, 1.1)\n",
    "            ax.set_ylim(-0.05, 1)\n",
    "        if dim == 2:\n",
    "            for i in range(k_clusters):\n",
    "                plt.scatter(x[y == i, 0], x[y == i, 1], s=3, label=f\"Cluster{i}\", alpha=0.4)\n",
    "\n",
    "def predict(igmn, n_samples, min, max, filter_n):\n",
    "    x = np.linspace(min, max, n_samples)\n",
    "    y = [igmn.predict([x_i], filter_n=filter_n) for x_i in x]\n",
    "    return x.reshape((-1,)), y\n",
    "\n",
    "def plot_gmm(X, Y, x_min, x_max, x, y):\n",
    "    \"\"\"Plot the density function based on samples from a GMM.\"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "\n",
    "    # plot the original data\n",
    "    # ax.scatter(X, np.zeros_like(X), s=10, alpha=0.1, label='Data')\n",
    "    k_clusters = len(np.unique(Y))\n",
    "    for i in range(k_clusters):\n",
    "        ax.scatter(X[Y == i], np.zeros_like(X[Y == i]), s=10, label=f\"Cluster{i}\", alpha=0.4)\n",
    "\n",
    "    \n",
    "    # plot the GMM density function\n",
    "    ax.plot(x, y, color=\"green\", alpha=0.2)\n",
    "    # ax.set_ylim(-0.05, 1)\n",
    "    ax.set_xlim(x_min, x_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGMN:\n",
    "    \"\"\"Incremental Gaussian Mixture Network\n",
    "\n",
    "    Implements the IGMN algorithm as described in:\n",
    "        - https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0139931&type=printable\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): dimension of the input data\n",
    "        beta (float): confidence level for the chi-square test\n",
    "        all_adults_criterion (bool): whether to use the all_adults_criterion\n",
    "        age_min (int): minimum age for a component to be considered an adult\n",
    "        acc_min (float): minimum accumulation for a component to be considered an adult\n",
    "        max_components (int): maximum number of components in the model\n",
    "        rank_type (str): type of covariance matrix to use, either \"full\" or \"diag\"\n",
    "        closest_n (int): number of closest components to merge into\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, beta, all_adults_criterion, age_min, acc_min, max_components, rank_type, closest_n):\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # hyperparameters\n",
    "        self.beta = beta\n",
    "        self.all_adults_criterion = all_adults_criterion\n",
    "        self.age_min = age_min\n",
    "        self.acc_min = acc_min\n",
    "        self.max_components = max_components\n",
    "        self.rank = rank_type\n",
    "        self.closest_n = closest_n\n",
    "\n",
    "        # components\n",
    "        self.components = []\n",
    "\n",
    "    def create_new_component(self, x, components):\n",
    "        \"\"\"Create a new component with the given input x\n",
    "\n",
    "        As described in section 2.2 (Creating new components)\n",
    "        \"\"\"\n",
    "        mu = x\n",
    "        acc = 1\n",
    "        age = 1\n",
    "\n",
    "        # p => mixing coefficient\n",
    "        if len(components) > 0:\n",
    "            p = 1 / (sum([comp['acc'] for comp in components]) + 1)\n",
    "        else:\n",
    "            p = 1\n",
    "\n",
    "        sigma_ini = 0.01\n",
    "        sigma = sigma_ini * np.eye(len(x)) # equation 13\n",
    "        return {\"mu\": mu, \"sigma\": sigma, \"p\": p, \"acc\": acc, \"age\": age}\n",
    "\n",
    "    \n",
    "    def remove_spurious_components(self):\n",
    "        \"\"\"A component j is removed whenever u_j > u_min and sp_j < sp_min, where u_min and sp_min are user-defined thresholds.\n",
    "\n",
    "        In that case, also, p(k) must be adjusted for all k in K (with k != j).\n",
    "        In other words, each component is given some tim eu_min to show its importance to the model in the form of an accumulation of its posterior probability sp_j\n",
    "\n",
    "        As described in section 2.3 (Removing spurious components)\n",
    "        \"\"\"\n",
    "        for j, component_j in enumerate(self.components):\n",
    "            if component_j['age'] > self.age_min and component_j['acc'] < self.acc_min:\n",
    "                # print(\"Removing component \", j)\n",
    "                self.components.pop(j)\n",
    "        self.update_priors()\n",
    "\n",
    "    def squared_mahalanobis_distance(self, x, mu, cov):\n",
    "        \"\"\"Squared Mahalanobis distance for a multivariate gaussian distribution\"\"\"\n",
    "        sigma_inv = np.linalg.inv(cov)\n",
    "        diff = x - mu # error vector\n",
    "        d_sq = diff.T @ sigma_inv @ diff\n",
    "        return d_sq.item()\n",
    "    \n",
    "    def pdf(self, x, mu, cov):\n",
    "        \"\"\"Multivariate Gaussian PDF\"\"\"\n",
    "        D = self.input_dim\n",
    "        d_sq = self.squared_mahalanobis_distance(x, mu, cov) # equation 1\n",
    "        norm_constant = 1 / np.sqrt((2 * np.pi) ** D * np.linalg.det(cov)) \n",
    "        return norm_constant * np.exp(-0.5 * d_sq) # equation 2\n",
    "\n",
    "    def update_with_x(self, component_j, x):\n",
    "        \"\"\"Equations 2 to 12 in the paper\"\"\"\n",
    "        denom = 0\n",
    "        for comp in self.components:\n",
    "            p_x_k = self.pdf(x, comp['mu'], comp['sigma'])\n",
    "            denom += p_x_k * comp['p']\n",
    "\n",
    "        p_x_j = self.pdf(x, component_j['mu'], component_j['sigma']) # equation 2\n",
    "        p_j_x = (p_x_j * component_j['p']) / denom # equation 3\n",
    "        component_j['age'] = component_j['age'] + 1 # equation 4\n",
    "        component_j['acc'] = component_j['acc'] + p_j_x # equation 5\n",
    "        e_j = x - component_j['mu'] # equation 6\n",
    "        omega_j = p_j_x / component_j['acc'] # equation 7\n",
    "        delta_mu_j = omega_j * e_j # equation 8\n",
    "        component_j['mu'] = component_j['mu'] + delta_mu_j # equation 9\n",
    "        e_j2 = x - component_j['mu'] # equation 10\n",
    "\n",
    "        \n",
    "        copy = component_j[\"sigma\"].copy()\n",
    "\n",
    "        # clip delta_mu_j to avoid making the covariance matrix non positive definite\n",
    "        delta_mu_j = np.clip(delta_mu_j, -0.01, 0.01) # TODO: find a better way to clip this value\n",
    "        # analysis of why this can get non-positive:\n",
    "        # term 1: 1-omega_j is a scalar, copy is a matrix -> term is positive definite\n",
    "        # term 2: + (e_j2 @ e_j2.T) \n",
    "        #               1. outer-product of a vector with itself -> positive definite, \n",
    "        #               2. multiplication by scalar omega_j -> positive definite,\n",
    "        #               3. addition of two positive definite matrices -> positive definite\n",
    "        # term 3: - (delta_mu_j @ delta_mu_j.T)\n",
    "        #               1. outer-product of a vector with itself -> positive definite\n",
    "        #               2. BUT subtraction of two positive definite matrices -> not necessarily positive definite\n",
    "        # BUT if delta_mu_j is too large, it can make the matrix non-positive definite\n",
    "        # I currently clip delta_mu_j to avoid this issue, but it is not the best solution as I am losing information\n",
    "        component_j[\"sigma\"] = \\\n",
    "            (1 - omega_j) * copy \\\n",
    "            + omega_j * (e_j2 @ e_j2.T) \\\n",
    "            - delta_mu_j @ delta_mu_j.T # equation 11\n",
    "        \n",
    "        # do not use full rank covariance matrix, only keep variances\n",
    "        if self.rank == \"diag\":\n",
    "            component_j[\"sigma\"] = np.diag(np.diag(component_j[\"sigma\"]))\n",
    "        \n",
    "        if not np.all(np.linalg.eigvals(component_j[\"sigma\"]) > 0):\n",
    "            raise ValueError(\"Sigma is not positive definite\")\n",
    "        \n",
    "        component_j['p'] = component_j['acc'] / sum([comp['acc'] for comp in self.components]) # equation 12\n",
    "\n",
    "    def update_priors(self):\n",
    "        total_acc = sum([comp['acc'] for comp in self.components])\n",
    "        for comp in self.components:\n",
    "            comp['p'] = comp['acc'] / total_acc\n",
    "\n",
    "    def update(self, x):\n",
    "        # section 2.2 - creating new components\n",
    "        if not self.components:\n",
    "            # print(\"No components yet, creating new one\")\n",
    "            new_component = self.create_new_component(x, self.components)\n",
    "            self.components.append(new_component)\n",
    "            return\n",
    "        \n",
    "        # section 2.3 - removing spurious components\n",
    "        self.remove_spurious_components()\n",
    "        \n",
    "        # section 2.2 - learning\n",
    "        updated = False\n",
    "        chi_sq_threshold = chi2.ppf(1-self.beta, self.input_dim)\n",
    "        for component_j in self.components:\n",
    "\n",
    "            d_sq = self.squared_mahalanobis_distance(x, \n",
    "                                                     component_j['mu'], \n",
    "                                                     component_j['sigma']) # equation 1)\n",
    "            \n",
    "            # check if d_sq is negative and throw error otherwise\n",
    "            if d_sq < 0:\n",
    "                raise ValueError(f\"Negative squared Mahalanobis distance: {d_sq} based on x = {x}, mu = {component_j['mu']}, sigma = {component_j['sigma']}\")\n",
    "\n",
    "            if d_sq < chi_sq_threshold:\n",
    "                self.update_with_x(component_j, x)\n",
    "                updated = True\n",
    "\n",
    "        # criterion 2: only create a new neuron if all neurons of that model have an age greater than the parameter age_min (taken from follow-up paper -> IGMN-NSE paper)\n",
    "        all_adults = all([comp['age'] > self.age_min for comp in self.components]) if self.all_adults_criterion else True\n",
    "        \n",
    "        if not updated and all_adults:\n",
    "            if len(self.components) < self.max_components:\n",
    "                new_component = self.create_new_component(x, self.components)\n",
    "                self.components.append(new_component)\n",
    "            else:\n",
    "                self.merge_to_closest_n(x)\n",
    "\n",
    "        self.update_priors()\n",
    "\n",
    "    def merge_to_closest_n(self, x):\n",
    "        distances = []\n",
    "        for component in self.components:\n",
    "            d_sq = self.squared_mahalanobis_distance(x, \n",
    "                                                     component['mu'], \n",
    "                                                     component['sigma'])\n",
    "\n",
    "            distances.append(d_sq)\n",
    "\n",
    "        # sort the components by distance\n",
    "        sorted_components = [comp for _, comp in sorted(zip(distances, self.components))]\n",
    "        closest_components = sorted_components[:self.closest_n]\n",
    "\n",
    "        for comp in closest_components:\n",
    "            self.update_with_x(comp, x)\n",
    "\n",
    "\n",
    "    def predict(self, x, filter_n=None):\n",
    "        components = self.components\n",
    "        if filter_n:\n",
    "            components = sorted(components, key=lambda c: c[\"p\"], reverse=True)[:filter_n].copy()\n",
    "\n",
    "            total_acc = sum([comp['acc'] for comp in components])\n",
    "            for comp in components:\n",
    "                comp['p'] = comp['acc'] / total_acc\n",
    "        \n",
    "        prob = 0\n",
    "        for comp in components:\n",
    "            p_x_j = self.pdf(x, comp['mu'], comp['sigma'])\n",
    "            p = comp['p']\n",
    "            prob += p_x_j * p\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: the scale matters a lot, if the scale is too small (between 0 and 1, the predictions do not make sense)\n",
    "X, Y = gen_data(k=3, dim=1, points_per_cluster=100, lim=[0, 100], spread=[1, 2])\n",
    "\n",
    "# shuffle X and Y (but keep the correspondence)\n",
    "p = np.random.permutation(len(X))\n",
    "X, Y = X[p], Y[p]\n",
    "plot_data(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igmn = IGMN(input_dim=1, beta=0.000001, all_adults_criterion=True, age_min=5.0, acc_min=3.0, max_components=20, rank_type = \"diag\", closest_n=20)\n",
    "idx = 0\n",
    "looped = 0\n",
    "\n",
    "for i in range(2000+1):\n",
    "    # next sample\n",
    "    x = X[idx]\n",
    "    \n",
    "    # print every 100th iteration\n",
    "    #if (idx % 100) == 0:\n",
    "    #    print(\"Iteration: \", (len(X) * looped) + idx)\n",
    "\n",
    "    # update the model with new sample\n",
    "    igmn.update(x)\n",
    "\n",
    "    # update index\n",
    "    idx += 1\n",
    "    # loop again over indices if we reached the end\n",
    "    if idx == len(X):\n",
    "        idx = 0\n",
    "        looped += 1\n",
    "\n",
    "print(\"Number of components: \", len(igmn.components))\n",
    "print(\"Components centered at: \", [comp['mu'].item() for comp in igmn.components])\n",
    "\n",
    "x_min, x_max = min(X)-1, max(X)+1\n",
    "n_samples = 100000\n",
    "x, y = predict(igmn, n_samples, x_min, x_max, filter_n=None)\n",
    "\n",
    "print(\"IGMN has \", len(igmn.components), \" components\")\n",
    "print(\"GMM integral: \", scipy.integrate.simpson(y, x), \" or \", scipy.integrate.quad(lambda x: igmn.predict(x), -50, 150))\n",
    "plot_gmm(X, Y, x_min, x_max, x, y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wetrek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
